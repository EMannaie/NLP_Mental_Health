# -*- coding: utf-8 -*-
"""Copy of Sentiment Analysis for Mental Health Monitoring

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZrTRvPiht0ZpDNmg8IzhZhASIR2ani8M
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
suchintikasarkar_sentiment_analysis_for_mental_health_path = kagglehub.dataset_download('suchintikasarkar/sentiment-analysis-for-mental-health')

print('Data source import complete.')

# Commented out IPython magic to ensure Python compatibility.
# import libraries

import pandas as pd
import numpy as np
import string
import spacy
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
import seaborn as sns
# %matplotlib inline
import re
from sklearn.naive_bayes import BernoulliNB
from nltk.corpus import stopwords
from nltk.stem import RSLPStemmer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
import joblib
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
import lightgbm as lgb
from multiprocessing import Pool, cpu_count
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras import Model
# Download necessary NLTK data files
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')



import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""**Background and Motivation**

Mental health is a fundamental aspect of overall well-being, affecting how individuals think, feel, and act. With the increasing prevalence of mental health disorders such as depression, anxiety, and stress, there is a growing need for innovative solutions to monitor and address these issues proactively. Traditional methods of mental health assessment often rely on self-reported surveys and clinical evaluations, which can be time-consuming and may not capture real-time fluctuations in an individual's emotional state.

**Objective**

This project aims to harness the power of sentiment analysis, a subset of natural language processing (NLP), to monitor mental health through textual data. Sentiment analysis involves the use of algorithms and machine learning techniques to identify and extract subjective information from text, categorizing it as positive, negative, or neutral. By applying sentiment analysis to text data from various sources, we can gain valuable insights into the emotional and mental states of individuals.

In today's fast-paced world, mental health has emerged as a critical area of concern, impacting millions globally. The advent of digital communication, particularly through social media and online forums, has provided a unique window into the collective psyche of individuals, offering valuable insights into their emotional and mental states. Our project leverages sentiment analysis, a powerful tool in the field of natural language processing (NLP), to monitor and assess mental health trends through textual data.

The primary objective of this project is to develop a robust model that can accurately classify text based on sentiment, helping to identify early signs of mental health issues such as anxiety, depression, and stress. By analyzing data from various sources, including social media posts, forums, and mental health support groups, our model aims to provide real-time insights into the emotional well-being of individuals.

Through this project, we aim to demonstrate how sentiment analysis can be an effective tool in mental health monitoring, offering a proactive approach to mental health care. By identifying negative sentiment patterns early, we can potentially facilitate timely interventions and support, ultimately contributing to better mental health outcomes.

# import the file and Let's begin !
"""

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/Combined Data.csv')
df.head(10)

df.tail(10)

print(df.isnull().sum())

df = df.dropna()
print(df.isnull().sum())

df['status'].nunique()

"""Let us understand the data"""

sentiment_counts=df['status'].value_counts()
print(sentiment_counts)

sentiment_counts.plot(kind='bar', title='Distribution of Sentiments')

df['status'].unique()

"""This will be our target for classifying text into the above groups. Once our model is trained with the data, we can use it to correctly classify new text input into these categories and take preventive actions to help the people with these mental health issues."""

df.shape

df.info()

df.describe()

"""Drop these rows with missing values"""

# Calculate the length of each statement
df['statement_length'] = df['statement'].apply(len)

# Display basic statistics of statement lengths
print(df['statement_length'].describe())

# Plot the distribution of statement lengths
df['statement_length'].hist(bins=100)
plt.title('Distribution of Statement Lengths')
plt.xlabel('Length of Statements')
plt.ylabel('Frequency')
plt.show()

"""let us get a more clear picture of this column excluding the outliers"""

# Calculate Q1 (25th percentile) and Q3 (75th percentile)
Q1 = df['statement_length'].quantile(0.25)
Q3 = df['statement_length'].quantile(0.75)
IQR = Q3 - Q1

# Define the lower and upper bound for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out the outliers
filtered_df = df[(df['statement_length'] >= lower_bound) & (df['statement_length'] <= upper_bound)]
# Plot the distribution of statement lengths without outliers
filtered_df['statement_length'].hist(bins=100)
plt.title('Distribution of Statement Lengths (Without Outliers)')
plt.xlabel('Length of Statements')
plt.ylabel('Frequency')
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Create a function to generate and display a word cloud
def generate_word_cloud(text, title):
    wordcloud = WordCloud(width=800, height=400).generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.show()

# Generate word clouds for each status
statuses = df['status'].unique()

for status in statuses:
    status_text = ' '.join(df[df['status'] == status]['statement'])
    generate_word_cloud(status_text, title=f'Word Cloud for {status}')

"""**Preprocess Text**
Text Preprocessing is traditionally an important step for Natural Language Processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.

taking a sample of the dataset since the dataset is huge and takes lots of time and energy to process.
"""

# Taking a sample of the dataframe, e.g., 20,000 rows
sample_size = 20000
df_sample = df.sample(n=sample_size, random_state=1)

# Load the spacy model
nlp = spacy.load("en_core_web_sm")

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

     # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Process text with spacy
    doc = nlp(text)

    # Lemmatize and remove stop words
    tokens = [token.lemma_ for token in doc if not token.is_stop]

    # Join the tokens back into a single string
    return ' '.join(tokens)

def preprocess_texts(texts):
    return [preprocess_text(text) for text in texts]

# Split the data into batches for multiprocessing
num_cores = cpu_count()
df_split = np.array_split(df_sample, num_cores)

# Create a multiprocessing Pool
with Pool(num_cores) as pool:
    # Preprocess the text in parallel
    results = pool.map(preprocess_texts, [batch['statement'].tolist() for batch in df_split])

# Combine the results
df_sample['cleaned_statement'] = [item for sublist in results for item in sublist]

# Display the first few rows of the DataFrame to confirm the changes
print(df_sample[['statement', 'cleaned_statement']].head())

df_sample.head()

# Extract features and labels
processedtext = df_sample['cleaned_statement']
sentiment = df_sample['status']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,
                                                    test_size=0.05, random_state=0)

print(f'X_train size: {len(X_train)}')
print(f'X_test size: {len(X_test)}')
print(f'y_train size: {len(y_train)}')
print(f'y_test size: {len(y_test)}')

"""TF-IDF Vectoriser

TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It combines two metrics:

Term Frequency (TF): Measures how frequently a term appears in a document.
Inverse Document Frequency (IDF): Assesses the importance of a term by measuring how common or rare it is across the entire corpus.
The TF-IDF score increases with the number of times a word appears in a document but is offset by the frequency of the word in the corpus. This helps to highlight important words that are unique to a document while down-weighting common words.

Why Use TF-IDF?
Feature Extraction: Converts text data into numerical features suitable for machine learning algorithms.
Highlighting Relevance: Emphasizes unique and significant terms while reducing the weight of common terms.
Text Analysis: Widely used in text classification, information retrieval, and natural language processing tasks.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Preprocess the text to include POS tags
def preprocess_text_with_pos(text):
    text = text.lower().translate(str.maketrans('', '', string.punctuation))
    doc = nlp(text)
    tokens_with_pos = [f"{token.text}_{token.pos_}" for token in doc if not token.is_stop]
    return ' '.join(tokens_with_pos)

# Add a new column with POS-tagged text
df_sample['statement_with_pos'] = df_sample['statement'].apply(preprocess_text_with_pos)

# TF-IDF for the original text
tfidf_vectorizer_original = TfidfVectorizer(max_features=5000)
tfidf_original = tfidf_vectorizer_original.fit_transform(df_sample['cleaned_statement'])

# TF-IDF for POS-tagged text
tfidf_vectorizer_pos = TfidfVectorizer(max_features=5000)
tfidf_pos = tfidf_vectorizer_pos.fit_transform(df_sample['statement_with_pos'])

# prompt: to this code add word embeddings and n grams

import gensim.downloader as api
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import TfidfVectorizer

# Load pre-trained word embeddings (e.g., GloVe)
try:
    word_vectors = api.load("glove-twitter-25")
except Exception as e:
    print(f"Error loading GloVe: {e}")
    print("Attempting to download...")
    !pip install --upgrade gensim
    word_vectors = api.load("glove-twitter-25")


def get_embedding(text):
    embeddings = []
    for word in text.split():
        if word in word_vectors:
            embeddings.append(word_vectors[word])
    if embeddings:
        return np.mean(embeddings, axis=0)
    else:
        return np.zeros(word_vectors.vector_size)  # Return a zero vector if no words are found


# Apply word embeddings to the preprocessed text
df_sample['text_embeddings'] = df_sample['cleaned_statement'].apply(get_embedding)


# N-gram features
ngram_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)  # Unigrams and bigrams
ngram_features = ngram_vectorizer.fit_transform(df_sample['cleaned_statement'])

# Combine features (embeddings and n-grams) if needed
# You might need to convert the embeddings to a sparse matrix for efficient combination
# from scipy.sparse import csr_matrix
# embedding_matrix = csr_matrix(np.array(df_sample['text_embeddings'].tolist()))
# combined_features = hstack([embedding_matrix, ngram_features])

# Example of using the combined features (if you created combined_features)
# X_train, X_test, y_train, y_test = train_test_split(combined_features, sentiment, test_size=0.2, random_state=42)

# prompt: # Load GloVe embeddings
# def load_glove_embeddings(file_path):
#     embeddings = {}
#     with open(file_path, 'r', encoding='utf-8') as f:
#         for line in f:
#             values = line.split()
#             word = values[0]
#             vector = np.array(values[1:], dtype='float32')
#             embeddings[word] = vector
#     return embeddings
# glove_embeddings = load_glove_embeddings('/path/to/glove.6B.100d.txt')
# # Generate GloVe embeddings for the statements
# def convert_to_glove_embeddings(cleaned_statements, embeddings):
#     sentence_embeddings = []
#     for sentence in cleaned_statements:
#         words = sentence.split()
#         word_vectors = [embeddings[word] for word in words if word in embeddings]
#         if word_vectors:
#             sentence_embeddings.append(np.mean(word_vectors, axis=0))
#         else:
#             sentence_embeddings.append(np.zeros(100))
#     return np.array(sentence_embeddings)
# glove_embeddings_features = convert_to_glove_embeddings(df_sample['cleaned_statement'], glove_embeddings)
# FIX THE CODE

import numpy as np
from gensim.models import Word2Vec

# Load GloVe embeddings
def load_glove_embeddings(file_path):
    embeddings = {}
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            vector = np.array(values[1:], dtype='float32')
            embeddings[word] = vector
    return embeddings

# Assuming you have the file path to your GloVe embeddings
glove_file_path = '/path/to/glove.6B.100d.txt'  # Replace with the actual path

try:
    glove_embeddings = load_glove_embeddings(glove_file_path)
    print("GloVe embeddings loaded successfully.")
except FileNotFoundError:
    print(f"Error: GloVe embeddings file not found at '{glove_file_path}'. Please provide the correct path.")
    glove_embeddings = {}  # Initialize as an empty dictionary to avoid further errors
except Exception as e:
    print(f"An error occurred while loading GloVe embeddings: {e}")
    glove_embeddings = {}


# Generate GloVe embeddings for the statements
def convert_to_glove_embeddings(cleaned_statements, embeddings):
    sentence_embeddings = []
    for sentence in cleaned_statements:
        words = sentence.split()
        word_vectors = [embeddings[word] for word in words if word in embeddings]
        if word_vectors:
            sentence_embeddings.append(np.mean(word_vectors, axis=0))
        else:
            sentence_embeddings.append(np.zeros(100)) # Use a zero vector if no words are found in the embeddings
    return np.array(sentence_embeddings)

# Assuming df_sample is your DataFrame and 'cleaned_statement' is the column with preprocessed text
if 'cleaned_statement' in df_sample.columns:
    glove_embeddings_features = convert_to_glove_embeddings(df_sample['cleaned_statement'], glove_embeddings)
else:
    print("Error: 'cleaned_statement' column not found in df_sample. Please ensure the column exists.")
    glove_embeddings_features = np.array([]) # Initialize as an empty array to avoid further errors

# prompt: from scipy.sparse import hstack
# from sklearn.preprocessing import StandardScaler
# from gensim.models import Word2Vec
# import numpy as np
# # ... (Your existing code) ...
# # Standardize embeddings before combining with sparse TF-IDF features
# if skip_gram_embeddings.size > 0 and len(skip_gram_embeddings.shape) > 1: #check if skip-gram embeddings are valid
#   scaler = StandardScaler()
#   skip_gram_embeddings_scaled = scaler.fit_transform(skip_gram_embeddings)
# else:
#   print("Warning: skip-gram embeddings are empty or invalid. Using zeros instead.")
#   skip_gram_embeddings_scaled = np.zeros((len(df_sample), 100)) # Placeholder
# if glove_embeddings_features.size > 0 and len(glove_embeddings_features.shape) > 1: #check if glove embeddings are valid
#   glove_embeddings_scaled = scaler.fit_transform(glove_embeddings_features)
# else:
#   print("Warning: GloVe embeddings are empty or invalid. Using zeros instead.")
#   glove_embeddings_scaled = np.zeros((len(df_sample), 100)) # Placeholder
# # Combine sparse matrices (TF-IDF) with dense embeddings
# combined_features = hstack([
#     tfidf_original,
#     tfidf_pos,
#     skip_gram_embeddings_scaled,
#     glove_embeddings_scaled
# ])
# # Train/test split
# from sklearn.model_selection import train_test_split
# X_train, X_test, y_train, y_test = train_test_split(combined_features, df_sample['target'], test_size=0.2, random_state=42)
# fix the code

from scipy.sparse import hstack
from sklearn.preprocessing import StandardScaler
from gensim.models import Word2Vec
import numpy as np

# ... (Your existing code) ...

# Standardize embeddings before combining with sparse TF-IDF features
if skip_gram_embeddings.size > 0 and len(skip_gram_embeddings.shape) > 1:  # check if skip-gram embeddings are valid
    scaler = StandardScaler()
    skip_gram_embeddings_scaled = scaler.fit_transform(skip_gram_embeddings)
else:
    print("Warning: skip-gram embeddings are empty or invalid. Using zeros instead.")
    skip_gram_embeddings_scaled = np.zeros((len(df_sample), 100))  # Placeholder

if glove_embeddings_features.size > 0 and len(glove_embeddings_features.shape) > 1:  # check if glove embeddings are valid
    glove_embeddings_scaled = scaler.fit_transform(glove_embeddings_features)
else:
    print("Warning: GloVe embeddings are empty or invalid. Using zeros instead.")
    glove_embeddings_scaled = np.zeros((len(df_sample), 100))  # Placeholder

# Combine sparse matrices (TF-IDF) with dense embeddings
# Ensure all arrays have the same number of rows
num_samples = len(df_sample)
if tfidf_original.shape[0] != num_samples:
    print("Warning: tfidf_original has inconsistent shape. Resizing...")
    tfidf_original = tfidf_original[:num_samples]

if tfidf_pos.shape[0] != num_samples:
    print("Warning: tfidf_pos has inconsistent shape. Resizing...")
    tfidf_pos = tfidf_pos[:num_samples]

if skip_gram_embeddings_scaled.shape[0] != num_samples:
    print("Warning: skip_gram_embeddings_scaled has inconsistent shape. Resizing...")
    skip_gram_embeddings_scaled = skip_gram_embeddings_scaled[:num_samples]

if glove_embeddings_scaled.shape[0] != num_samples:
    print("Warning: glove_embeddings_scaled has inconsistent shape. Resizing...")
    glove_embeddings_scaled = glove_embeddings_scaled[:num_samples]

combined_features = hstack([
    tfidf_original,
    tfidf_pos,
    skip_gram_embeddings_scaled,
    glove_embeddings_scaled
])

# prompt: # Train/test split
# from sklearn.model_selection import train_test_split
# X_train, X_test, y_train, y_test = train_test_split(combined_features, df_sample['target'], test_size=0.2, random_state=42)
# fix the code

from scipy.sparse import hstack
from sklearn.preprocessing import StandardScaler
from gensim.models import Word2Vec

# ... (Your existing code) ...

# Standardize embeddings before combining with sparse TF-IDF features
if skip_gram_embeddings.size > 0 and len(skip_gram_embeddings.shape) > 1:  # check if skip-gram embeddings are valid
    scaler = StandardScaler()
    skip_gram_embeddings_scaled = scaler.fit_transform(skip_gram_embeddings)
else:
    print("Warning: skip-gram embeddings are empty or invalid. Using zeros instead.")
    skip_gram_embeddings_scaled = np.zeros((len(df_sample), 100))  # Placeholder

if glove_embeddings_features.size > 0 and len(glove_embeddings_features.shape) > 1:  # check if glove embeddings are valid
    glove_embeddings_scaled = scaler.fit_transform(glove_embeddings_features)
else:
    print("Warning: GloVe embeddings are empty or invalid. Using zeros instead.")
    glove_embeddings_scaled = np.zeros((len(df_sample), 100))  # Placeholder

# Combine sparse matrices (TF-IDF) with dense embeddings
# Ensure all arrays have the same number of rows
num_samples = len(df_sample)
if tfidf_original.shape[0] != num_samples:
    print("Warning: tfidf_original has inconsistent shape. Resizing...")
    tfidf_original = tfidf_original[:num_samples]

if tfidf_pos.shape[0] != num_samples:
    print("Warning: tfidf_pos has inconsistent shape. Resizing...")
    tfidf_pos = tfidf_pos[:num_samples]

if skip_gram_embeddings_scaled.shape[0] != num_samples:
    print("Warning: skip_gram_embeddings_scaled has inconsistent shape. Resizing...")
    skip_gram_embeddings_scaled = skip_gram_embeddings_scaled[:num_samples]

if glove_embeddings_scaled.shape[0] != num_samples:
    print("Warning: glove_embeddings_scaled has inconsistent shape. Resizing...")
    glove_embeddings_scaled = glove_embeddings_scaled[:num_samples]

combined_features = hstack([
    tfidf_original,
    tfidf_pos,
    skip_gram_embeddings_scaled,
    glove_embeddings_scaled
])

# Train/test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(combined_features, df_sample['status'], test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Train the model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Use SMOTE to oversample the training data
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=0)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Define the parameter grid
param_grid = {
    'alpha': [0.1, 0.5, 1.0, 5.0, 10.0]
}

# Initialize the model
bnb = BernoulliNB()

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=bnb, param_grid=param_grid, cv=5, scoring='accuracy')

# Fit GridSearchCV
grid_search.fit(X_train_resampled, y_train_resampled)

# Print the best parameters and best score
print(f'Best Parameters: {grid_search.best_params_}')
print(f'Best Score: {grid_search.best_score_}')

# Train the model with the best parameters
best_bnb = grid_search.best_estimator_
best_bnb.fit(X_train_resampled, y_train_resampled)

# Predict on the test set
y_pred_best_bnb = best_bnb.predict(X_test)

# Evaluate the model
print("Tuned Bernoulli Naive Bayes")
print(classification_report(y_test, y_pred_best_bnb))

"""Building Models

we got only 56 percent accuracy from the model BernoulliNB

### Conclusion

In this notebook, we tackled a sentiment analysis problem using the Bernoulli Naive Bayes model. Here is a summary of our approach and findings:

1. **Data Preprocessing**:
   - We began by preprocessing our text data, which included cleaning the text, removing noise, and transforming the text data into numerical features using the TF-IDF vectorizer. We optimized the TF-IDF vectorizer parameters to capture important n-grams and limit the feature space.

2. **Handling Class Imbalance**:
   - To address the class imbalance in our dataset, we employed the Synthetic Minority Over-sampling Technique (SMOTE). This helped in balancing the training data by oversampling the minority classes, thus improving the model's ability to learn from these classes.

3. **Model Selection and Hyperparameter Tuning**:
   - We trained the Bernoulli Naive Bayes model and conducted hyperparameter tuning using GridSearchCV to find the optimal `alpha` parameter.

4. **Model Evaluation**:
   - The model was evaluated using metrics such as precision, recall, F1-score, and accuracy.
   - The Bernoulli Naive Bayes model achieved an accuracy of 56%. While this shows the model has some predictive capability, it also indicates significant room for improvement.

### Findings

- **Bernoulli Naive Bayes**:
  - The Bernoulli Naive Bayes model, after hyperparameter tuning and class balancing, showed moderate performance. While it managed to achieve an accuracy of 56%, it struggled with accurately predicting some of the minority classes, highlighting the challenges inherent in dealing with imbalanced datasets.

### Future Work

To further improve the performance of our model, we can consider the following steps:
- **Feature Engineering**: Experiment with additional features such as word embeddings (e.g., Word2Vec, GloVe) or more advanced vectorization techniques like BERT embeddings.
- **Model Exploration**: Try other advanced models such as deep learning models (LSTM, CNN) or transformer-based models (BERT, RoBERTa) that have shown great success in text classification tasks.
- **Hyperparameter Tuning**: Conduct a more exhaustive search for hyperparameters using techniques like RandomizedSearchCV or Bayesian optimization.
- **Ensemble Methods**: Combine multiple models to create an ensemble that could leverage the strengths of different models for improved performance.

### Final Remarks

This project provided valuable insights into the challenges and solutions for sentiment analysis using machine learning models. By systematically preprocessing the data, handling class imbalance, and selecting appropriate models, we were able to build a classifier that can predict sentiments from text data with a moderate degree of accuracy. The journey from data preprocessing to model evaluation underscores the importance of each step in building robust machine learning models and sets the stage for future improvements and exploration.
"""
